{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T09:33:23.930647Z",
     "iopub.status.busy": "2025-01-09T09:33:23.930363Z",
     "iopub.status.idle": "2025-01-09T09:33:23.935846Z",
     "shell.execute_reply": "2025-01-09T09:33:23.934793Z",
     "shell.execute_reply.started": "2025-01-09T09:33:23.930624Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T09:31:31.259429Z",
     "iopub.status.busy": "2025-01-09T09:31:31.259139Z",
     "iopub.status.idle": "2025-01-09T09:31:31.275354Z",
     "shell.execute_reply": "2025-01-09T09:31:31.274584Z",
     "shell.execute_reply.started": "2025-01-09T09:31:31.259401Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time of execution: 2025-01-09_09-31-31\n"
     ]
    }
   ],
   "source": [
    "now = datetime.datetime.now()\n",
    "now = now.strftime('%Y-%m-%d_%H-%M-%S')\n",
    "print(f\"Time of execution: {now}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T09:31:31.276902Z",
     "iopub.status.busy": "2025-01-09T09:31:31.276607Z",
     "iopub.status.idle": "2025-01-09T09:31:31.290405Z",
     "shell.execute_reply": "2025-01-09T09:31:31.289576Z",
     "shell.execute_reply.started": "2025-01-09T09:31:31.276881Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CURRENT DIRECTORY:  /kaggle/working\n",
      "DATASET DIRECTORY:  /kaggle/input/defectxray\n"
     ]
    }
   ],
   "source": [
    "CURRENT_DIR = os.getcwd()\n",
    "print('CURRENT DIRECTORY: ', CURRENT_DIR)\n",
    "\n",
    "DATASET_DIR = '/kaggle/input/defectxray' if CURRENT_DIR.split('/')[1] == 'kaggle' else CURRENT_DIR\n",
    "print('DATASET DIRECTORY: ', DATASET_DIR)\n",
    "\n",
    "config = {\n",
    "        'name': \"gdxray\",\n",
    "        'data_dir': os.path.join(DATASET_DIR, \"data/gdxray\"),\n",
    "        'metadata': os.path.join(DATASET_DIR, \"metadata\"),\n",
    "        'subset': \"train\",\n",
    "        'labels': True,\n",
    "        'device': \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        'image_size': (224, 224),\n",
    "        'learning_rate': 3e-4,\n",
    "        'batch_size': 8,\n",
    "        'epochs': 10,\n",
    "        'save_dir': os.path.join(CURRENT_DIR, \"logs/gdxray\")\n",
    "   }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather all function in other file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T09:31:31.411662Z",
     "iopub.status.busy": "2025-01-09T09:31:31.411434Z",
     "iopub.status.idle": "2025-01-09T09:31:31.450280Z",
     "shell.execute_reply": "2025-01-09T09:31:31.449541Z",
     "shell.execute_reply.started": "2025-01-09T09:31:31.411644Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SAM(nn.Module):\n",
    "    def __init__(self, bias=False):\n",
    "        super(SAM, self).__init__()\n",
    "        self.bias = bias\n",
    "        self.conv = nn.Conv2d(in_channels=2, out_channels=1, kernel_size=7, stride=1, padding=3, dilation=1, bias=self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        max = torch.max(x,1)[0].unsqueeze(1)\n",
    "        avg = torch.mean(x,1).unsqueeze(1)\n",
    "        concat = torch.cat((max,avg), dim=1)\n",
    "        output = self.conv(concat)\n",
    "        output = F.sigmoid(output) * x\n",
    "        return output\n",
    "\n",
    "class CAM(nn.Module):\n",
    "    def __init__(self, channels, r):\n",
    "        super(CAM, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.r = r\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(in_features=self.channels, out_features=self.channels//self.r, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=self.channels//self.r, out_features=self.channels, bias=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        max = F.adaptive_max_pool2d(x, output_size=1)\n",
    "        avg = F.adaptive_avg_pool2d(x, output_size=1)\n",
    "        b, c, _, _ = x.size()\n",
    "        linear_max = self.linear(max.view(b,c)).view(b, c, 1, 1)\n",
    "        linear_avg = self.linear(avg.view(b,c)).view(b, c, 1, 1)\n",
    "        output = linear_max + linear_avg\n",
    "        output = F.sigmoid(output) * x\n",
    "        return output\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, channels, r):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.r = r\n",
    "        self.sam = SAM(bias=False)\n",
    "        self.cam = CAM(channels=self.channels, r=self.r)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.cam(x)\n",
    "        output = self.sam(output)\n",
    "        return output\n",
    "\n",
    "class ResHDCCBAM(nn.Module):\n",
    "    def __init__(self, in_channels, output_channels, r=16):\n",
    "        super(ResHDCCBAM, self).__init__()\n",
    "\n",
    "        self.branch1 = nn.Conv2d(in_channels, output_channels, kernel_size=3, padding=1, dilation=1)\n",
    "\n",
    "        self.branch2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, dilation=1),\n",
    "            nn.Conv2d(in_channels, output_channels, kernel_size=3, padding=2, dilation=2)\n",
    "        )\n",
    "\n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, dilation=1),\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=2, dilation=2),\n",
    "            nn.Conv2d(in_channels, output_channels, kernel_size=3, padding=4, dilation=4)\n",
    "        )\n",
    "\n",
    "        self.cbam = CBAM(in_channels, r)\n",
    "        self.conv1 = nn.Conv2d(in_channels, output_channels, kernel_size=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels, output_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out1 = self.branch1(x)\n",
    "        out2 = self.branch2(x)\n",
    "        out3 = self.branch3(x)\n",
    "        cbam = self.cbam(x)\n",
    "        out = out1 + out2 + out3 + self.conv1(cbam) + self.conv2(residual)\n",
    "        return out\n",
    "\n",
    "class DownSampling(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DownSampling, self).__init__()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "def SimAM(X, lamb):\n",
    "    # spatial size\n",
    "    n = X.shape[2] * X.shape[3]- 1\n",
    "    # square of (t- u)\n",
    "    d = (X- X.mean(dim=[2,3])).pow(2)\n",
    "    # d.sum() / n is channel variance\n",
    "    v = d.sum(dim=[2,3]) / n\n",
    "    # E_inv groups all importance of X\n",
    "    E_inv = d / (4 * (v + lamb)) + 0.5\n",
    "    # return attended features\n",
    "    return X * nn.Sigmoid(E_inv)\n",
    "\n",
    "class ResSimAM(nn.Module):\n",
    "    def __init__(self, in_channels, lamb):\n",
    "        super(ResSimAM, self).__init__()\n",
    "\n",
    "        self.lamb = lamb\n",
    "        self.conv1 = ConvBNReLU(in_channels, in_channels, 3, 1, 1, 1)\n",
    "        self.conv2 = ConvBNReLU(in_channels, in_channels, 3, 1, 1, 1, use_relu=False)\n",
    "\n",
    "        self.simam = SimAM\n",
    "\n",
    "    def forward(self, X):\n",
    "        residual = X\n",
    "        X = self.conv1(X)\n",
    "        X = self.conv2(X)\n",
    "        X = self.simam(X, self.lamb)\n",
    "        return nn.ReLU(X + residual)\n",
    "\n",
    "class DCIM(nn.Module):\n",
    "    def __init__(self, output_list, num_parallel, r=16):\n",
    "        super(DCIM, self).__init__()\n",
    "        self.levels = len(output_list)\n",
    "        self.num_parallel = num_parallel\n",
    "\n",
    "        # Initialize convolutional, upsampling, and downsampling layers\n",
    "        self.H = nn.ModuleDict()\n",
    "        self.D = nn.ModuleDict()\n",
    "        self.U = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        for k in range(self.num_parallel):\n",
    "            for level in range(self.levels):\n",
    "                idx = f\"{level}_{k}\"\n",
    "                input_channels = output_list[level] if level != 0 else 0 # Downsampled input channels\n",
    "                input_channels += 0 if k == 0 or level == self.levels - 1 else output_list[level] + output_list[level+1] # Skip connection and Upsampling\n",
    "\n",
    "                input_channels = output_list[level] if input_channels == 0 else input_channels\n",
    "\n",
    "                # print(f\"Level {l}, Branch {k}, Input channels: {input_channels}\")\n",
    "\n",
    "                self.H[idx] = ResHDCCBAM(input_channels, output_list[level], r)\n",
    "\n",
    "                if level < self.levels - 1:\n",
    "                    # self.D[idx] = DownSampling(input_channels, output_list[l+1])\n",
    "                    self.D[idx] = DownSampling(output_list[level], output_list[level+1])\n",
    "                # if k < self.num_parallel - 1:\n",
    "                #     self.U[f\"{l+1}_{k}\"] = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initilize tensor storage\n",
    "        X = {}\n",
    "\n",
    "        for k in range(self.num_parallel):\n",
    "            for lvl in range(self.levels):\n",
    "                idx = f\"{lvl}_{k}\"\n",
    "\n",
    "                if lvl == 0:\n",
    "                    if k == 0:\n",
    "                        X[idx] = self.H[idx](x)\n",
    "                    else:\n",
    "                        U = self.U(X[f\"{lvl+1}_{k-1}\"])\n",
    "                        X[idx] = self.H[idx](torch.cat([X[f\"{lvl}_{k-1}\"], U], dim = 1))\n",
    "\n",
    "                elif lvl > 0 and lvl < self.levels - 1:\n",
    "                    if k == 0:\n",
    "                        D = self.D[f\"{lvl-1}_{k}\"](X[f\"{lvl-1}_{k}\"])\n",
    "                        X[idx] = self.H[idx](D)\n",
    "                    else:\n",
    "                        D = self.D[f\"{lvl-1}_{k}\"](X[f\"{lvl-1}_{k}\"])\n",
    "                        U = self.U(X[f\"{lvl+1}_{k-1}\"])\n",
    "                        X[idx] = self.H[idx](torch.cat([X[f\"{lvl}_{k-1}\"], D, U], dim = 1))\n",
    "                elif lvl == self.levels - 1:\n",
    "                    D = self.D[f\"{lvl-1}_{k}\"](X[f\"{lvl-1}_{k}\"])\n",
    "                    X[idx] = self.H[idx](D)\n",
    "\n",
    "        return [X[f\"{lvl}_{self.num_parallel-1}\"] for lvl in range(self.levels)]\n",
    "\n",
    "class AFF(nn.Module):\n",
    "    \"\"\"\n",
    "    Adaptive Feature Fusion (AFF) module for semantic segmentation\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(AFF, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x2 = nn.Upsample(size=x1.size()[2:], mode='bilinear', align_corners=True)(x2)\n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "class CARAFE(nn.Module):\n",
    "    # CARAFE: Content-Aware ReAssembly of FEatures \"https://arxiv.org/pdf/1905.02188.pdf\"\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        input_channels (int): input feature channels\n",
    "        scale_factor (int): upsample ratio\n",
    "        up_kernel (int): kernel size of CARAFE op\n",
    "        up_group (int): group size of CARAFE op\n",
    "        encoder_kernel (int): kernel size of content encoder\n",
    "        encoder_dilation (int): dilation of content encoder\n",
    "\n",
    "    Returns:\n",
    "        upsampled feature map\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels, scale_factor=2, kernel_up=5, kernel_encoder=3):\n",
    "        super(CARAFE, self).__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.kernel_up = kernel_up\n",
    "        self.kernel_encoder = kernel_encoder\n",
    "        self.down = nn.Conv2d(input_channels, input_channels // 4, 1)\n",
    "        self.encoder = nn.Conv2d(input_channels // 4, self.scale_factor ** 2 * self.kernel_up ** 2,self.kernel_encoder, 1, self.kernel_encoder // 2)\n",
    "        self.out = nn.Conv2d(input_channels, input_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.size()\n",
    "        # N,C,H,W -> N,C,delta*H,delta*W\n",
    "        # kernel prediction module\n",
    "        kernel_tensor = self.down(x)  # (N, Cm, H, W)\n",
    "        kernel_tensor = self.encoder(kernel_tensor)  # (N, S^2 * Kup^2, H, W)\n",
    "        kernel_tensor = F.pixel_shuffle(kernel_tensor, self.scale_factor)  # (N, S^2 * Kup^2, H, W)->(N, Kup^2, S*H, S*W)\n",
    "        kernel_tensor = F.softmax(kernel_tensor, dim=1)  # (N, Kup^2, S*H, S*W)\n",
    "        kernel_tensor = kernel_tensor.unfold(2, self.scale_factor, step=self.scale_factor) # (N, Kup^2, H, W*S, S)\n",
    "        kernel_tensor = kernel_tensor.unfold(3, self.scale_factor, step=self.scale_factor) # (N, Kup^2, H, W, S, S)\n",
    "        kernel_tensor = kernel_tensor.reshape(N, self.kernel_up ** 2, H, W, self.scale_factor ** 2) # (N, Kup^2, H, W, S^2)\n",
    "        kernel_tensor = kernel_tensor.permute(0, 2, 3, 1, 4)  # (N, H, W, Kup^2, S^2)\n",
    "\n",
    "        # content-aware reassembly module\n",
    "        # tensor.unfold: dim, size, step\n",
    "        x = F.pad(x, pad=(self.kernel_up // 2, self.kernel_up // 2,\n",
    "                                          self.kernel_up // 2, self.kernel_up // 2),\n",
    "                          mode='constant', value=0) # (N, C, H+Kup//2+Kup//2, W+Kup//2+Kup//2)\n",
    "        x = x.unfold(2, self.kernel_up, step=1) # (N, C, H, W+Kup//2+Kup//2, Kup)\n",
    "        x = x.unfold(3, self.kernel_up, step=1) # (N, C, H, W, Kup, Kup)\n",
    "        x = x.reshape(N, C, H, W, -1) # (N, C, H, W, Kup^2)\n",
    "        x = x.permute(0, 2, 3, 1, 4)  # (N, H, W, C, Kup^2)\n",
    "\n",
    "        out_tensor = torch.matmul(x, kernel_tensor)  # (N, H, W, C, S^2)\n",
    "        out_tensor = out_tensor.reshape(N, H, W, -1)\n",
    "        out_tensor = out_tensor.permute(0, 3, 1, 2)\n",
    "        out_tensor = F.pixel_shuffle(out_tensor, self.scale_factor)\n",
    "\n",
    "        out_tensor = self.out(out_tensor)\n",
    "        #print(\"up shape:\",out_tensor.shape)\n",
    "        return out_tensor\n",
    "\n",
    "class ConvBNReLU(nn.Module):\n",
    "    '''Module for the Conv-BN-ReLU tuple.'''\n",
    "    def __init__(self, c_in, c_out, kernel_size, stride, padding, dilation,\n",
    "                 use_relu=True):\n",
    "        super(ConvBNReLU, self).__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "                c_in, c_out, kernel_size=kernel_size, stride=stride,\n",
    "                padding=padding, dilation=dilation, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(c_out)\n",
    "        if use_relu:\n",
    "            self.relu = nn.ReLU(inplace=True)\n",
    "        else:\n",
    "            self.relu = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        if self.relu is not None:\n",
    "            x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class AdaptiveThresholdPrediction(nn.Module):\n",
    "    def __init__(self, in_channels=64, pool_size=(1, 1), num_classes=2):\n",
    "        \"\"\"\n",
    "        Adaptive Threshold Prediction Module.\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels.\n",
    "            intermediate_channels (int): Number of intermediate channels (e.g., 64).\n",
    "            pool_size (tuple): Adaptive pooling output size (default 1x1).\n",
    "        returns:\n",
    "            torch.Tensor: Thresholded output tensor of the same shape as the input tensor.\n",
    "        \"\"\"\n",
    "        super(AdaptiveThresholdPrediction, self).__init__()\n",
    "\n",
    "        # First path: Conv1x1 -> STAF\n",
    "        self.conv1x1_main = nn.Conv2d(in_channels, num_classes, kernel_size=1)\n",
    "\n",
    "        # Second path: Adaptive Max Pool -> Conv1x1 -> Threshold prediction\n",
    "        self.adaptive_pool = nn.AdaptiveMaxPool2d(output_size=pool_size)\n",
    "        self.conv1x1_thresh = nn.Conv2d(in_channels, num_classes, kernel_size=1)\n",
    "\n",
    "        # Learnable scalar for threshold prediction\n",
    "        self.sigma = nn.Parameter(torch.tensor(1.0))  # Scale factor (learnable)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the Adaptive Threshold Prediction Module.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (B, C, H, W)\n",
    "        Returns:\n",
    "            torch.Tensor: Thresholded output tensor.\n",
    "        \"\"\"\n",
    "        # First path\n",
    "        main_output = self.conv1x1_main(x)  # Output: (B, 64, H, W)\n",
    "\n",
    "        # Second path for threshold prediction\n",
    "        pooled_output = self.adaptive_pool(x)  # Output: (B, C, 1, 1)\n",
    "        threshold = self.conv1x1_thresh(pooled_output)  # Output: (B, 64, 1, 1)\n",
    "        threshold = torch.sigmoid(threshold) * self.sigma  # Sigmoid to predict Thresh\n",
    "\n",
    "        # Broadcast threshold to match main_output shape\n",
    "        threshold = F.interpolate(threshold, size=main_output.shape[2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Apply threshold (modulation or scaling)\n",
    "        output = main_output * threshold\n",
    "\n",
    "        assert output.shape[1] == 2, f\"Output shape: {output.shape}\"\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class WResHDC_FF(nn.Module):\n",
    "    def __init__(self, num_classes, input_channels, output_list, num_parallel, channel_ratio=16, upsample_cfg=dict(type='carafe', scale_factor = 2, kernel_up = 5, kernel_encoder = 3, compress_channels = 64)):\n",
    "        \"\"\"\n",
    "        The overall architecture of the WResHDC-FF models for semantic segmentation.\n",
    "        It contains the following components:\n",
    "            1. ResHDCCBAM: Residual Hierarchical Dense Convolutional Channel Attention Block\n",
    "            2. AFF: Adaptive Feature Fusion\n",
    "            3. CARAFE: Content-Aware ReAssembly of FEatures\n",
    "\n",
    "        Args:\n",
    "            num_classes (int): Number of classes for the segmentation task\n",
    "            output_list (list): List of output channels for each level\n",
    "            num_parallel (int): Number of parallel branches in each level\n",
    "            r (int): Reduction ratio for the channel attention module\n",
    "            upsample_cfg (dict): Configuration for the upsampling module\n",
    "        \"\"\"\n",
    "        super(WResHDC_FF, self).__init__()\n",
    "\n",
    "        self.levels = len(output_list) # 5\n",
    "        self.num_parallel = num_parallel # 1\n",
    "        self.channel_ratio = channel_ratio\n",
    "\n",
    "        # First convolutional layer as compress layer\n",
    "        # self.conv1 = ConvBNReLU(input_channels, output_list[0], 3, 1, 1, 1)\n",
    "        self.conv1 = nn.Conv2d(input_channels, output_list[0], kernel_size=1, stride=1)\n",
    "\n",
    "        # Initialize DCIM module\n",
    "        self.dcim = DCIM(output_list, num_parallel)\n",
    "\n",
    "        self.aff = nn.ModuleList()\n",
    "        # Initialize AFF module\n",
    "        for level in range(self.levels - 2):\n",
    "            self.aff.append(AFF(output_list[level] + output_list[level+1], output_list[level]))\n",
    "\n",
    "        # Initialize CARAFE module\n",
    "        self.upsample = nn.ModuleList()\n",
    "        self.convbnrelu1 = nn.ModuleList()\n",
    "        self.convbnrelu2 = nn.ModuleList()\n",
    "        for level in range(self.levels - 1, 0, -1):\n",
    "            if upsample_cfg['type'] == 'carafe':\n",
    "                self.upsample.append(CARAFE(output_list[level],\n",
    "                                            upsample_cfg['scale_factor'],\n",
    "                                            upsample_cfg['kernel_up'],\n",
    "                                            upsample_cfg['kernel_encoder']))\n",
    "            else:\n",
    "                self.upsample.append(nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True))\n",
    "\n",
    "            self.convbnrelu1.append(ConvBNReLU(output_list[level] + output_list[level-1], output_list[level], 3, 1, 1, 1))\n",
    "            self.convbnrelu2.append(ConvBNReLU(output_list[level], output_list[level-1], 3, 1, 1, 1))\n",
    "\n",
    "        # Adaptive Threshold Prediction Module\n",
    "        self.atp = AdaptiveThresholdPrediction(in_channels=output_list[0], pool_size=(1, 1), num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)  # 3*320*320 -> 64*320*320\n",
    "        # DCIM module\n",
    "        X = self.dcim(x) # levels of feature maps\n",
    "        F = X.copy()\n",
    "        # delete two last levels\n",
    "        del F[self.levels-1]\n",
    "\n",
    "        # Adaptive Feature Fusion module\n",
    "        for level in range(self.levels - 2): # 0, 1, 2\n",
    "            F[level] = self.aff[level](X[level], X[level+1])\n",
    "\n",
    "        # Upsampling module\n",
    "        # out = self.upsample[0](X[self.levels - 1]) # W*H*1024 -> 2W*2H*1024\n",
    "        # out = torch.cat([out, X[self.levels - 2]], dim=1)\n",
    "        # out = self.convbnrelu1[0](out)\n",
    "        # out = self.convbnrelu2[0](out) # 2W*2H*1024 -> 2W*2H*512\n",
    "\n",
    "        # out = self.upsample[1](out) # 2W*2H*512 -> 4W*4H*512\n",
    "        # out = torch.cat([out, F[self.levels - 3]], dim=1)\n",
    "        # out = self.convbnrelu1[1](out)\n",
    "        # out = self.convbnrelu2[1](out) # 4W*4H*512 -> 4W*4H*256\n",
    "\n",
    "        # out = self.upsample[2](out)\n",
    "        # out = torch.cat([out, F[self.levels - 4]], dim=1)\n",
    "        # out = self.convbnrelu1[2](out)\n",
    "        # out = self.convbnrelu2[2](out)\n",
    "\n",
    "        # out = self.upsample[3](out)\n",
    "        # out = torch.cat([out, F[self.levels - 5]], dim=1)\n",
    "        # out = self.convbnrelu1[3](out)\n",
    "        # out = self.convbnrelu2[3](out)\n",
    "\n",
    "        out = X[self.levels - 1]\n",
    "        for level in range(self.levels - 1, 0, -1):\n",
    "            out = self.upsample[self.levels - 1 - level](out)\n",
    "            out = torch.cat([out, F[level-1]], dim=1)\n",
    "            out = self.convbnrelu1[self.levels - 1 - level](out)\n",
    "            out = self.convbnrelu2[self.levels - 1 - level](out)\n",
    "\n",
    "        return self.atp(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T09:31:31.528603Z",
     "iopub.status.busy": "2025-01-09T09:31:31.528371Z",
     "iopub.status.idle": "2025-01-09T09:31:31.550008Z",
     "shell.execute_reply": "2025-01-09T09:31:31.549163Z",
     "shell.execute_reply.started": "2025-01-09T09:31:31.528575Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.utils import draw_segmentation_masks\n",
    "\n",
    "BACKGROUND_CLASS = 0\n",
    "WELDING_DEFECT = 1\n",
    "OBJECT_CLASSES = [BACKGROUND_CLASS, WELDING_DEFECT]\n",
    "\n",
    "\n",
    "# Base Configuration Class\n",
    "# Don't use this class directly. Instead, sub-class it and override\n",
    "# the configurations you need to change.\n",
    "\n",
    "\n",
    "class Config(object):\n",
    "    \"\"\"Base configuration class. For custom configurations, create a\n",
    "    sub-class that inherits from this one and override properties\n",
    "    that need to be changed.\n",
    "    \"\"\"\n",
    "\n",
    "    # Name the configurations. For example, 'COCO', 'Experiment 3', ...etc.\n",
    "    # Useful if your code needs to do things differently depending on which\n",
    "    # experiment is running.\n",
    "    NAME = None  # Override in sub-classes\n",
    "\n",
    "    # NUMBER OF GPUs to use. For CPU training, use 1\n",
    "    GPU_COUNT = 1\n",
    "\n",
    "    # Number of images to train with on each GPU. A 12GB GPU can typically\n",
    "    # handle 2 images of 1024x1024px.\n",
    "    # Adjust based on your GPU memory and image sizes. Use the highest\n",
    "    # number that your GPU can handle for best performance.\n",
    "    IMAGES_PER_GPU = 2\n",
    "\n",
    "    # Number of training steps per epoch\n",
    "    # This doesn't need to match the size of the training set. Tensorboard\n",
    "    # updates are saved at the end of each epoch, so setting this to a\n",
    "    # smaller number means getting more frequent TensorBoard updates.\n",
    "    # Validation stats are also calculated at each epoch end and they\n",
    "    # might take a while, so don't set this too small to avoid spending\n",
    "    # a lot of time on validation stats.\n",
    "    STEPS_PER_EPOCH = 1000\n",
    "\n",
    "    # Number of validation steps to run at the end of every training epoch.\n",
    "    # A bigger number improves accuracy of validation stats, but slows\n",
    "    # down the training.\n",
    "    VALIDATION_STEPS = 50\n",
    "\n",
    "    # Number of classification classes (including background)\n",
    "    NUM_CLASSES = 2  # Override in sub-classes\n",
    "\n",
    "    # If enabled, resizes instance masks to a smaller size to reduce\n",
    "    # memory load. Recommended when using high-resolution images.\n",
    "    USE_MINI_MASK = True\n",
    "    MINI_MASK_SHAPE = (128, 128)  # (height, width) of the mini-mask\n",
    "\n",
    "    # Input image resing\n",
    "    # Images are resized such that the smallest side is >= IMAGE_MIN_DIM and\n",
    "    # the longest side is <= IMAGE_MAX_DIM. In case both conditions can't\n",
    "    # be satisfied together the IMAGE_MAX_DIM is enforced.\n",
    "    IMAGE_MIN_DIM = 800\n",
    "    IMAGE_MAX_DIM = 512\n",
    "    # If True, pad images with zeros such that they're (max_dim by max_dim)\n",
    "    IMAGE_PADDING = True  # currently, the False option is not supported\n",
    "\n",
    "    # Image mean (RGB)\n",
    "    MEAN_PIXEL = np.array([123.7, 116.8, 103.9])\n",
    "\n",
    "    # Learning rate and momentum\n",
    "    # The Mask RCNN paper uses lr=0.02, but on TensorFlow it causes\n",
    "    # weights to explode. Likely due to differences in optimzer\n",
    "    # implementation.\n",
    "    LEARNING_RATE = 0.001\n",
    "    LEARNING_MOMENTUM = 0.9\n",
    "\n",
    "    # Weight decay regularization\n",
    "    WEIGHT_DECAY = 0.0001\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Set values of computed attributes.\"\"\"\n",
    "        # Effective batch size\n",
    "        self.BATCH_SIZE = self.IMAGES_PER_GPU * self.GPU_COUNT\n",
    "\n",
    "        # Input image size\n",
    "        self.IMAGE_SHAPE = np.array([self.IMAGE_MAX_DIM, self.IMAGE_MAX_DIM, 3])\n",
    "\n",
    "        # Compute backbone size from input image size\n",
    "        self.BACKBONE_SHAPES = np.array(\n",
    "            [\n",
    "                [\n",
    "                    int(math.ceil(self.IMAGE_SHAPE[0] / stride)),\n",
    "                    int(math.ceil(self.IMAGE_SHAPE[1] / stride)),\n",
    "                ]\n",
    "                for stride in self.BACKBONE_STRIDES\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def display(self):\n",
    "        \"\"\"Display Configuration values.\"\"\"\n",
    "        print(\"\\nConfigurations:\")\n",
    "        for a in dir(self):\n",
    "            if not a.startswith(\"__\") and not callable(getattr(self, a)):\n",
    "                print(\"{:30} {}\".format(a, getattr(self, a)))\n",
    "        print(\"\\n\")\n",
    "\n",
    "class GDXrayDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset of Xray Images\n",
    "\n",
    "    Images are referred to using their image_id (relative path to image).\n",
    "    An example image_id is: \"Weldings/W0001/W0001_0004.png\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: dict, labels: bool, transform):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            config (dict): Contain nessesary information for the dataset\n",
    "            config = {\n",
    "                'name': str, # Name of the dataset\n",
    "                'data_dir': str, # Path to the data directory\n",
    "                'subset': str, # 'train' or 'val'\n",
    "                'metadata': str, # Path to the metadata file\n",
    "                }\n",
    "            labels (bool): Whether to load labels\n",
    "            transform (callable, optional): Transform to be applied to the images.\n",
    "        \"\"\"\n",
    "\n",
    "        self.config = config\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "        # Initialize the dataset infos\n",
    "        self.image_info = []\n",
    "        self.image_indices = {}\n",
    "        self.class_info = [{\"source\": \"\", \"id\": 0, \"name\": \"BG\"}]\n",
    "\n",
    "        # Add classes\n",
    "        self.add_class(config[\"name\"], 1, \"Defect\")\n",
    "\n",
    "        # Load the dataset\n",
    "        metadata_img = \"{}/{}_{}.txt\".format(config['metadata'], config[\"name\"], \"images\")\n",
    "        metadata_label = \"{}/{}_{}.txt\".format(config['metadata'], config[\"name\"], \"labels\") if labels else None\n",
    "\n",
    "        # Load image ids from key 'image' in dictionary in metadata file\n",
    "        image_ids = []\n",
    "        image_ids.extend(self.load_metadata(metadata_img, \"image\"))\n",
    "        if self.labels:\n",
    "            label_ids = []\n",
    "            label_ids.extend(self.load_metadata(metadata_label, \"label\"))\n",
    "\n",
    "        for i, image_id in enumerate(image_ids):\n",
    "            img_path = os.path.join(config[\"data_dir\"], image_id)\n",
    "            label_path = \"\"\n",
    "            if self.labels:\n",
    "                label_path = os.path.join(config[\"data_dir\"], label_ids[i])\n",
    "\n",
    "                if not os.path.exists(label_path):\n",
    "                    print(\"Skipping \", image_id, \" Reason: No mask\")\n",
    "\n",
    "                    del self.image_ids[i]\n",
    "                    del self.label_ids[i]\n",
    "\n",
    "                    continue\n",
    "\n",
    "            print(\"Adding image: \", image_id)\n",
    "\n",
    "            self.add_image(config[\"name\"], config['subset'], image_id, img_path, label=label_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx (int): Index of the data to fetch.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, label) where both are transformed.\n",
    "        \"\"\"\n",
    "\n",
    "        image_path = self.image_info[idx][\"path\"]\n",
    "        # image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "        # self.update_info(idx, height_org=image.shape[0], width_org=image.shape[1])\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        width, height = image.size\n",
    "        self.update_info(idx, height_org=width, width_org=height)\n",
    "\n",
    "        if self.labels:\n",
    "            label_path = self.image_info[idx][\"label\"]\n",
    "            # label = cv2.imread(label_path, cv2.IMREAD_GRAYSCALE)\n",
    "            label = Image.open(label_path).convert('L') # read as grayscale\n",
    "\n",
    "        if self.transform:\n",
    "            image, label = self.transform(image, label) if self.labels else self.transform(image)\n",
    "\n",
    "        if self.labels:\n",
    "            return image, label\n",
    "\n",
    "    def add_class(self, source, class_id, class_name):\n",
    "        assert \".\" not in source, \"Source name cannot contain a dot\"\n",
    "        # Does the class exist already?\n",
    "        for info in self.class_info:\n",
    "            if info[\"source\"] == source and info[\"id\"] == class_id:\n",
    "                # source.class_id combination already available, skip\n",
    "                return\n",
    "        # Add the class\n",
    "        self.class_info.append(\n",
    "            {\n",
    "                \"source\": source,\n",
    "                \"id\": class_id,\n",
    "                \"name\": class_name,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def add_image(self, source, subset, image_id, path, **kwargs):\n",
    "        image_info = {\n",
    "            \"id\": image_id,\n",
    "            \"subset\": subset,\n",
    "            \"source\": source,\n",
    "            \"path\": path,\n",
    "        }\n",
    "        image_info.update(kwargs)\n",
    "        self.image_info.append(image_info)\n",
    "        self.image_indices[image_id] = len(self.image_info) - 1\n",
    "\n",
    "    def update_info(self, image_id, **kwargs):\n",
    "        info = self.image_info[image_id]\n",
    "        info.update(kwargs)\n",
    "        self.image_info[image_id] = info\n",
    "\n",
    "    def load_metadata(self, metadata, key):\n",
    "        \"\"\"\n",
    "        metadata file has the following format:\n",
    "        {\n",
    "            \"image\": [<image_id>, <image_id>, ...],\n",
    "            \"label\": [<label_id>, <label_id>, ...]\n",
    "        }\n",
    "\n",
    "        Args:\n",
    "            metadata (str): Path to the metadata file\n",
    "            key (str): Key to load from the metadata file\n",
    "        \"\"\"\n",
    "\n",
    "        image_ids = []\n",
    "        with open(metadata, \"r\") as metadata_file:\n",
    "            image_ids += metadata_file.readlines()\n",
    "        return [p.rstrip() for p in image_ids]\n",
    "\n",
    "\n",
    "def visualize_samples(dataset, num_samples=3, labels=True):\n",
    "    \"\"\"\n",
    "    Visualize random samples from the dataset with images in the first column and labels in the second.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): The PyTorch Dataset to visualize.\n",
    "        num_samples (int): Number of random samples to visualize.\n",
    "    \"\"\"\n",
    "\n",
    "    # Print a message notifying that the transformation is applied\n",
    "    print(\"CAUTION!!!! Be careful with normalization...\") if dataset.transform is not None else None\n",
    "\n",
    "    # Randomly select indices\n",
    "    indices = random.sample(range(len(dataset)), num_samples)\n",
    "\n",
    "    # Set up the Matplotlib figure\n",
    "    fig, axes = plt.subplots(num_samples, 2, figsize=(10, 5 * num_samples))\n",
    "    if num_samples == 1:\n",
    "        axes = [axes]  # Ensure axes is iterable for a single sample\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        image, label = dataset[idx]\n",
    "        image_path = dataset.image_info[idx][\"path\"]\n",
    "        label_path = dataset.image_info[idx][\"label\"]\n",
    "\n",
    "        image, label = v2.ToTensor()(image), v2.ToTensor()(label)\n",
    "        image_mask = draw_segmentation_masks(image, label.bool(), alpha=0.5)\n",
    "\n",
    "        image = image.permute(1, 2, 0).numpy()\n",
    "        image_mask = image_mask.permute(1, 2, 0).numpy()\n",
    "\n",
    "        # Display the image\n",
    "        axes[i][0].imshow(image)\n",
    "        axes[i][0].set_title(\n",
    "            f\"Image: {image_path.split('/')[-1].split('.')[0]} -- Size: {image.shape}\"\n",
    "        )\n",
    "        axes[i][0].axis(\"off\")\n",
    "\n",
    "        # Display the label\n",
    "        axes[i][1].imshow(image_mask)\n",
    "        axes[i][1].set_title(\n",
    "            f\"Label: {label_path.split('/')[-1].split('.')[0]} -- Size: {label.shape}\"\n",
    "        )\n",
    "        axes[i][1].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_augmentations(dataset, idx=0, samples=10, cols=5):\n",
    "    dataset = copy.deepcopy(dataset)\n",
    "    # get transform from dataset and remove normalization\n",
    "    transform = dataset.transform.transforms[:-1]\n",
    "    dataset.transform = v2.Compose(transform)\n",
    "    rows = samples // cols\n",
    "    figure, ax = plt.subplots(nrows=rows, ncols=cols * 2, figsize=(12, 6))\n",
    "    for i in range(samples):\n",
    "        image, mask = dataset[idx]\n",
    "        image = image.permute(1, 2, 0).numpy()\n",
    "        mask = mask.permute(1, 2, 0).numpy()\n",
    "        ax.ravel()[i * 2].imshow(image)\n",
    "        ax.ravel()[i * 2].set_title(\"Image\")\n",
    "        ax.ravel()[i * 2].set_axis_off()\n",
    "        ax.ravel()[i * 2 + 1].imshow(mask, cmap='gray')\n",
    "        ax.ravel()[i * 2 + 1].set_title(\"Mask\")\n",
    "        ax.ravel()[i * 2 + 1].set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def save_metrics(save_dir, prefix, train_losses, train_dcs, valid_losses, valid_dcs):\n",
    "    metrics = {\n",
    "        'train_losses': train_losses,\n",
    "        'train_dcs': train_dcs,\n",
    "        'valid_losses': valid_losses,\n",
    "        'valid_dcs': valid_dcs\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(save_dir, f\"metrics_{prefix}.json\"), 'w') as f:\n",
    "        json.dump(metrics, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T09:31:31.551211Z",
     "iopub.status.busy": "2025-01-09T09:31:31.551007Z",
     "iopub.status.idle": "2025-01-09T09:31:31.577741Z",
     "shell.execute_reply": "2025-01-09T09:31:31.577036Z",
     "shell.execute_reply.started": "2025-01-09T09:31:31.551194Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def dice_coefficient(prediction, target, epsilon=1e-07):\n",
    "    prediction_copy = prediction.clone()\n",
    "\n",
    "    prediction_copy[prediction_copy < 0] = 0\n",
    "    prediction_copy[prediction_copy > 0] = 1\n",
    "\n",
    "    intersection = abs(torch.sum(prediction_copy * target))\n",
    "    union = abs(torch.sum(prediction_copy) + torch.sum(target))\n",
    "    dice = (2. * intersection + epsilon) / (union + epsilon)\n",
    "\n",
    "    return dice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T09:31:31.579406Z",
     "iopub.status.busy": "2025-01-09T09:31:31.579198Z",
     "iopub.status.idle": "2025-01-09T09:31:31.593191Z",
     "shell.execute_reply": "2025-01-09T09:31:31.592435Z",
     "shell.execute_reply.started": "2025-01-09T09:31:31.579388Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def make_one_hot(input, num_classes):\n",
    "    \"\"\"Convert class index tensor to one hot encoding tensor.\n",
    "\n",
    "    Args:\n",
    "         input: A tensor of shape [N, 1, *]\n",
    "         num_classes: An int of number of class\n",
    "    Returns:\n",
    "        A tensor of shape [N, num_classes, *]\n",
    "    \"\"\"\n",
    "    shape = np.array(input.shape)\n",
    "    shape[1] = num_classes\n",
    "    shape = tuple(shape)\n",
    "    result = torch.zeros(shape)\n",
    "    result = result.scatter_(1, input.cpu(), 1)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "class BinaryDiceLoss(nn.Module):\n",
    "    \"\"\"Dice loss of binary class\n",
    "    Args:\n",
    "        smooth: A float number to smooth loss, and avoid NaN error, default: 1\n",
    "        p: Denominator value: \\sum{x^p} + \\sum{y^p}, default: 2\n",
    "        predict: A tensor of shape [N, *]\n",
    "        target: A tensor of shape same with predict\n",
    "        reduction: Reduction method to apply, return mean over batch if 'mean',\n",
    "            return sum if 'sum', return a tensor of shape [N,] if 'none'\n",
    "    Returns:\n",
    "        Loss tensor according to arg reduction\n",
    "    Raise:\n",
    "        Exception if unexpected reduction\n",
    "    \"\"\"\n",
    "    def __init__(self, smooth=1, p=2, reduction='mean'):\n",
    "        super(BinaryDiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "        self.p = p\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, predict, target):\n",
    "        assert predict.shape[0] == target.shape[0], \"predict & target batch size don't match\"\n",
    "        predict = predict.contiguous().view(predict.shape[0], -1)\n",
    "        target = target.contiguous().view(target.shape[0], -1)\n",
    "\n",
    "        num = torch.sum(torch.mul(predict, target), dim=1) + self.smooth\n",
    "        den = torch.sum(predict.pow(self.p) + target.pow(self.p), dim=1) + self.smooth\n",
    "\n",
    "        loss = 1 - num / den\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        elif self.reduction == 'none':\n",
    "            return loss\n",
    "        else:\n",
    "            raise Exception('Unexpected reduction {}'.format(self.reduction))\n",
    "\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    \"\"\"Dice loss, need one hot encode input\n",
    "    Args:\n",
    "        weight: An array of shape [num_classes,]\n",
    "        ignore_index: class index to ignore\n",
    "        predict: A tensor of shape [N, C, *]\n",
    "        target: A tensor of same shape with predict\n",
    "        other args pass to BinaryDiceLoss\n",
    "    Return:\n",
    "        same as BinaryDiceLoss\n",
    "    \"\"\"\n",
    "    def __init__(self, weight=None, ignore_index=None, **kwargs):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.kwargs = kwargs\n",
    "        self.weight = weight\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, predict, target):\n",
    "        assert predict.shape == target.shape, 'predict & target shape do not match'\n",
    "        dice = BinaryDiceLoss(**self.kwargs)\n",
    "        total_loss = 0\n",
    "        predict = F.softmax(predict, dim=1)\n",
    "\n",
    "        for i in range(target.shape[1]):\n",
    "            if i != self.ignore_index:\n",
    "                dice_loss = dice(predict[:, i], target[:, i])\n",
    "                if self.weight is not None:\n",
    "                    assert self.weight.shape[0] == target.shape[1], \\\n",
    "                        'Expect weight shape [{}], get[{}]'.format(target.shape[1], self.weight.shape[0])\n",
    "                    dice_loss *= self.weights[i]\n",
    "                total_loss += dice_loss\n",
    "\n",
    "        return total_loss/target.shape[1]\n",
    "\n",
    "def dice_loss(output, target, weights=None, ignore_index=None):\n",
    "    \"\"\"\n",
    "    output : NxCxHxW Variable\n",
    "    target :  NxHxW LongTensor\n",
    "    weights : C FloatTensor\n",
    "    ignore_index : int index to ignore from loss\n",
    "    \"\"\"\n",
    "    eps = 0.0001\n",
    "\n",
    "    output = output.exp()\n",
    "    encoded_target = output.detach() * 0\n",
    "    if ignore_index is not None:\n",
    "        mask = target == ignore_index\n",
    "        target = target.clone()\n",
    "        target[mask] = 0\n",
    "        encoded_target.scatter_(1, target.unsqueeze(1), 1)\n",
    "        mask = mask.unsqueeze(1).expand_as(encoded_target)\n",
    "        encoded_target[mask] = 0\n",
    "    else:\n",
    "        encoded_target.scatter_(1, target.unsqueeze(1), 1)\n",
    "\n",
    "    if weights is None:\n",
    "        weights = 1\n",
    "\n",
    "    intersection = output * encoded_target\n",
    "    numerator = 2 * intersection.sum(0).sum(1).sum(1)\n",
    "    denominator = output + encoded_target\n",
    "\n",
    "    if ignore_index is not None:\n",
    "        denominator[mask] = 0\n",
    "    denominator = denominator.sum(0).sum(1).sum(1) + eps\n",
    "    loss_per_channel = weights * (1 - (numerator / denominator))\n",
    "\n",
    "    return loss_per_channel.sum() / output.size(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T09:31:31.594430Z",
     "iopub.status.busy": "2025-01-09T09:31:31.594243Z",
     "iopub.status.idle": "2025-01-09T09:31:31.605620Z",
     "shell.execute_reply": "2025-01-09T09:31:31.604888Z",
     "shell.execute_reply.started": "2025-01-09T09:31:31.594413Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "def torch_train_transform(size=(224, 224), scale=(0.08, 1.0), rotation=30, flip=0.5):\n",
    "    image_transforms = v2.Compose([\n",
    "        # v2.ToImage(),\n",
    "        v2.RandomCrop(size),  # Random crop of size 256x256\n",
    "        # v2.Resize(size),\n",
    "        v2.RandomRotation(degrees=rotation, interpolation=InterpolationMode.BILINEAR),  # Random rotation within 30 degrees\n",
    "        # v2.RandomResize(size=size, scale=scale),  # Random rescale and crop\n",
    "        v2.RandomAffine(degrees=15, translate=(0.1, 0.1)),  # Random affine transformation\n",
    "        v2.RandomHorizontalFlip(p=flip),  # Random horizontal flip\n",
    "        v2.GaussianBlur(kernel_size=3),  # Apply Gaussian blur\n",
    "        # v2.RandomErasing(p=0.5),  # Random cutout\n",
    "        v2.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Random color jitter\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        v2.ToTensor(),\n",
    "        v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    return image_transforms\n",
    "\n",
    "def torch_valid_transform(size=(224, 224)):\n",
    "    image_transforms = v2.Compose([\n",
    "        v2.Resize(size),\n",
    "        v2.PILToTensor(),\n",
    "        v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    return image_transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T09:31:31.606848Z",
     "iopub.status.busy": "2025-01-09T09:31:31.606601Z",
     "iopub.status.idle": "2025-01-09T09:31:31.862978Z",
     "shell.execute_reply": "2025-01-09T09:31:31.862129Z",
     "shell.execute_reply.started": "2025-01-09T09:31:31.606829Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding image:  welding/W0001/W0001_0000.png\n",
      "Adding image:  welding/W0001/W0001_0001.png\n",
      "Adding image:  welding/W0001/W0001_0002.png\n",
      "Adding image:  welding/W0001/W0001_0003.png\n",
      "Adding image:  welding/W0001/W0001_0004.png\n",
      "Adding image:  welding/W0001/W0001_0005.png\n",
      "Adding image:  welding/W0001/W0001_0006.png\n",
      "Adding image:  welding/W0001/W0001_0007.png\n",
      "Adding image:  welding/W0001/W0001_0008.png\n",
      "Adding image:  welding/W0001/W0001_0009.png\n",
      "Adding image:  welding/W0001/W0001_0010.png\n",
      "Adding image:  welding/W0001/W0001_0011.png\n",
      "Adding image:  welding/W0001/W0001_0012.png\n",
      "Adding image:  welding/W0001/W0001_0013.png\n",
      "Adding image:  welding/W0001/W0001_0014.png\n",
      "Adding image:  welding/W0001/W0001_0015.png\n",
      "Adding image:  welding/W0001/W0001_0016.png\n",
      "Adding image:  welding/W0001/W0001_0017.png\n",
      "Adding image:  welding/W0001/W0001_0018.png\n",
      "Adding image:  welding/W0001/W0001_0019.png\n",
      "Adding image:  welding/W0001/W0001_0020.png\n",
      "Adding image:  welding/W0001/W0001_0021.png\n",
      "Adding image:  welding/W0001/W0001_0022.png\n",
      "Adding image:  welding/W0001/W0001_0023.png\n",
      "Adding image:  welding/W0001/W0001_0024.png\n",
      "Adding image:  welding/W0001/W0001_0025.png\n",
      "Adding image:  welding/W0001/W0001_0026.png\n",
      "Adding image:  welding/W0001/W0001_0027.png\n",
      "Adding image:  welding/W0001/W0001_0028.png\n",
      "Adding image:  welding/W0001/W0001_0029.png\n",
      "Adding image:  welding/W0001/W0001_0030.png\n",
      "Adding image:  welding/W0001/W0001_0031.png\n",
      "Adding image:  welding/W0001/W0001_0032.png\n",
      "Adding image:  welding/W0001/W0001_0033.png\n",
      "Adding image:  welding/W0001/W0001_0034.png\n",
      "Adding image:  welding/W0001/W0001_0035.png\n",
      "Adding image:  welding/W0001/W0001_0036.png\n",
      "Adding image:  welding/W0001/W0001_0037.png\n",
      "Adding image:  welding/W0001/W0001_0038.png\n",
      "Adding image:  welding/W0001/W0001_0039.png\n",
      "Adding image:  welding/W0001/W0001_0040.png\n",
      "Adding image:  welding/W0001/W0001_0041.png\n",
      "Adding image:  welding/W0001/W0001_0042.png\n",
      "Adding image:  welding/W0001/W0001_0043.png\n",
      "Adding image:  welding/W0001/W0001_0044.png\n",
      "Adding image:  welding/W0001/W0001_0045.png\n",
      "Adding image:  welding/W0001/W0001_0046.png\n",
      "Adding image:  welding/W0001/W0001_0047.png\n",
      "Adding image:  welding/W0001/W0001_0048.png\n",
      "Adding image:  welding/W0001/W0001_0049.png\n",
      "Adding image:  welding/W0001/W0001_0050.png\n",
      "Adding image:  welding/W0001/W0001_0051.png\n",
      "Adding image:  welding/W0001/W0001_0052.png\n",
      "Adding image:  welding/W0001/W0001_0053.png\n",
      "Adding image:  welding/W0001/W0001_0054.png\n",
      "Adding image:  welding/W0001/W0001_0055.png\n",
      "Adding image:  welding/W0001/W0001_0056.png\n",
      "Adding image:  welding/W0001/W0001_0000.png\n",
      "Adding image:  welding/W0001/W0001_0001.png\n",
      "Adding image:  welding/W0001/W0001_0002.png\n",
      "Adding image:  welding/W0001/W0001_0003.png\n",
      "Adding image:  welding/W0001/W0001_0004.png\n",
      "Adding image:  welding/W0001/W0001_0005.png\n",
      "Adding image:  welding/W0001/W0001_0006.png\n",
      "Adding image:  welding/W0001/W0001_0007.png\n",
      "Adding image:  welding/W0001/W0001_0008.png\n",
      "Adding image:  welding/W0001/W0001_0009.png\n",
      "Adding image:  welding/W0001/W0001_0010.png\n",
      "Adding image:  welding/W0001/W0001_0011.png\n",
      "Adding image:  welding/W0001/W0001_0012.png\n",
      "Adding image:  welding/W0001/W0001_0013.png\n",
      "Adding image:  welding/W0001/W0001_0014.png\n",
      "Adding image:  welding/W0001/W0001_0015.png\n",
      "Adding image:  welding/W0001/W0001_0016.png\n",
      "Adding image:  welding/W0001/W0001_0017.png\n",
      "Adding image:  welding/W0001/W0001_0018.png\n",
      "Adding image:  welding/W0001/W0001_0019.png\n",
      "Adding image:  welding/W0001/W0001_0020.png\n",
      "Adding image:  welding/W0001/W0001_0021.png\n",
      "Adding image:  welding/W0001/W0001_0022.png\n",
      "Adding image:  welding/W0001/W0001_0023.png\n",
      "Adding image:  welding/W0001/W0001_0024.png\n",
      "Adding image:  welding/W0001/W0001_0025.png\n",
      "Adding image:  welding/W0001/W0001_0026.png\n",
      "Adding image:  welding/W0001/W0001_0027.png\n",
      "Adding image:  welding/W0001/W0001_0028.png\n",
      "Adding image:  welding/W0001/W0001_0029.png\n",
      "Adding image:  welding/W0001/W0001_0030.png\n",
      "Adding image:  welding/W0001/W0001_0031.png\n",
      "Adding image:  welding/W0001/W0001_0032.png\n",
      "Adding image:  welding/W0001/W0001_0033.png\n",
      "Adding image:  welding/W0001/W0001_0034.png\n",
      "Adding image:  welding/W0001/W0001_0035.png\n",
      "Adding image:  welding/W0001/W0001_0036.png\n",
      "Adding image:  welding/W0001/W0001_0037.png\n",
      "Adding image:  welding/W0001/W0001_0038.png\n",
      "Adding image:  welding/W0001/W0001_0039.png\n",
      "Adding image:  welding/W0001/W0001_0040.png\n",
      "Adding image:  welding/W0001/W0001_0041.png\n",
      "Adding image:  welding/W0001/W0001_0042.png\n",
      "Adding image:  welding/W0001/W0001_0043.png\n",
      "Adding image:  welding/W0001/W0001_0044.png\n",
      "Adding image:  welding/W0001/W0001_0045.png\n",
      "Adding image:  welding/W0001/W0001_0046.png\n",
      "Adding image:  welding/W0001/W0001_0047.png\n",
      "Adding image:  welding/W0001/W0001_0048.png\n",
      "Adding image:  welding/W0001/W0001_0049.png\n",
      "Adding image:  welding/W0001/W0001_0050.png\n",
      "Adding image:  welding/W0001/W0001_0051.png\n",
      "Adding image:  welding/W0001/W0001_0052.png\n",
      "Adding image:  welding/W0001/W0001_0053.png\n",
      "Adding image:  welding/W0001/W0001_0054.png\n",
      "Adding image:  welding/W0001/W0001_0055.png\n",
      "Adding image:  welding/W0001/W0001_0056.png\n"
     ]
    }
   ],
   "source": [
    "transform_train = torch_train_transform(config['image_size'])\n",
    "transform_valid = torch_valid_transform(config['image_size'])\n",
    "\n",
    "train_dataset = GDXrayDataset(config, labels=config['labels'], transform=transform_train)\n",
    "valid_dataset = GDXrayDataset(config, labels=config['labels'], transform=transform_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T09:31:31.864268Z",
     "iopub.status.busy": "2025-01-09T09:31:31.863976Z",
     "iopub.status.idle": "2025-01-09T09:31:31.867942Z",
     "shell.execute_reply": "2025-01-09T09:31:31.867100Z",
     "shell.execute_reply.started": "2025-01-09T09:31:31.864240Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if config['device'] == \"cuda\":\n",
    "    num_workers = torch.cuda.device_count() * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T09:31:31.868989Z",
     "iopub.status.busy": "2025-01-09T09:31:31.868696Z",
     "iopub.status.idle": "2025-01-09T09:31:31.882237Z",
     "shell.execute_reply": "2025-01-09T09:31:31.881409Z",
     "shell.execute_reply.started": "2025-01-09T09:31:31.868961Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset=train_dataset,\n",
    "                              num_workers=num_workers, pin_memory=False,\n",
    "                              batch_size=config['batch_size'],\n",
    "                              shuffle=True)\n",
    "valid_dataloader = DataLoader(dataset=valid_dataset,\n",
    "                              num_workers=num_workers, pin_memory=False,\n",
    "                              batch_size=config['batch_size'],\n",
    "                              shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model and opt and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T09:33:32.893315Z",
     "iopub.status.busy": "2025-01-09T09:33:32.893043Z",
     "iopub.status.idle": "2025-01-09T09:33:33.753412Z",
     "shell.execute_reply": "2025-01-09T09:33:33.752776Z",
     "shell.execute_reply.started": "2025-01-09T09:33:32.893295Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "output_list = [64, 128, 256, 512]\n",
    "num_parallel = 2\n",
    "num_classes = 2\n",
    "upsampling_cfg = dict(type='carafe', scale_factor=2, kernel_up=5, kernel_encoder=3)\n",
    "model = WResHDC_FF(num_classes, 3, output_list, num_parallel, upsample_cfg=upsampling_cfg)\n",
    "model.to(config['device'])\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'])\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T09:33:37.617938Z",
     "iopub.status.busy": "2025-01-09T09:33:37.617583Z",
     "iopub.status.idle": "2025-01-09T09:33:37.621841Z",
     "shell.execute_reply": "2025-01-09T09:33:37.620903Z",
     "shell.execute_reply.started": "2025-01-09T09:33:37.617912Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T09:33:41.565805Z",
     "iopub.status.busy": "2025-01-09T09:33:41.565462Z",
     "iopub.status.idle": "2025-01-09T09:33:42.557341Z",
     "shell.execute_reply": "2025-01-09T09:33:42.555766Z",
     "shell.execute_reply.started": "2025-01-09T09:33:41.565776Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 196.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 79.12 MiB is free. Process 2327 has 15.81 GiB memory in use. Of the allocated memory 15.42 GiB is allocated by PyTorch, and 105.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-9a8893f1fddb>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mdc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdice_coefficient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-ac1a7da9b993>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 3*320*320 -> 64*320*320\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;31m# DCIM module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdcim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# levels of feature maps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0mF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;31m# delete two last levels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-ac1a7da9b993>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    171\u001b[0m                         \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"{lvl-1}_{k}\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"{lvl-1}_{k}\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                         \u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"{lvl+1}_{k-1}\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                         \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"{lvl}_{k-1}\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mlvl\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlevels\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                     \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"{lvl-1}_{k}\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"{lvl-1}_{k}\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-ac1a7da9b993>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mout2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mout3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mcbam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcbam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mout2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mout3\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcbam\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresidual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-ac1a7da9b993>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-ac1a7da9b993>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mconcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 196.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 79.12 MiB is free. Process 2327 has 15.81 GiB memory in use. Of the allocated memory 15.42 GiB is allocated by PyTorch, and 105.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "train_dcs = []\n",
    "valid_losses = []\n",
    "valid_dcs = []\n",
    "\n",
    "for epoch in tqdm(range(config['epochs'])):\n",
    "    model.train()\n",
    "    train_running_loss = 0.0\n",
    "    train_running_dc = 0.0\n",
    "    for idx, img_mask in enumerate(tqdm(train_dataloader, position=0, leave=True)):\n",
    "        img = img_mask[0].to(config['device'], dtype=torch.float32)\n",
    "        mask = img_mask[1].to(config['device'], dtype=torch.float32)\n",
    "        y_pred = model(img)\n",
    "        optimizer.zero_grad()\n",
    "        dc = dice_coefficient(y_pred, mask)\n",
    "        loss = criterion(y_pred, mask)\n",
    "        train_running_loss += loss.item()\n",
    "        train_running_dc += dc.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss = train_running_loss / (idx + 1)\n",
    "    train_dc = train_running_dc / (idx + 1)\n",
    "    train_losses.append(train_loss)\n",
    "    train_dcs.append(train_dc)\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_running_dc = 0.0\n",
    "    with torch.no_grad():\n",
    "        for idx, img_mask in enumerate(tqdm(valid_dataloader, position=0, leave=True)):\n",
    "            img = img_mask[0].to(config['device'], dtype=torch.float32)\n",
    "            mask = img_mask[1].to(config['device'], dtype=torch.float32)\n",
    "            y_pred = model(img)\n",
    "            loss = criterion\n",
    "            dc = dice_coefficient(y_pred, mask)\n",
    "            val_running_loss += loss.item()\n",
    "            val_running_dc += dc.item()\n",
    "        val_loss = val_running_loss / (idx + 1)\n",
    "        val_dc = val_running_dc / (idx + 1)\n",
    "    valid_losses.append(val_loss)\n",
    "    valid_dcs.append(val_dc)\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Training Loss EPOCH {epoch + 1}: {train_loss:.4f}\")\n",
    "    print(f\"Training DICE EPOCH {epoch + 1}: {train_dc:.4f}\")\n",
    "    print(\"\\n\")\n",
    "    print(f\"Validation Loss EPOCH {epoch + 1}: {val_loss:.4f}\")\n",
    "    print(f\"Validation DICE EPOCH {epoch + 1}: {val_dc:.4f}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-09T09:31:35.455213Z",
     "iopub.status.idle": "2025-01-09T09:31:35.455486Z",
     "shell.execute_reply": "2025-01-09T09:31:35.455381Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(config['save_dir']):\n",
    "    os.makedirs(config['save_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-09T09:31:35.456246Z",
     "iopub.status.idle": "2025-01-09T09:31:35.456504Z",
     "shell.execute_reply": "2025-01-09T09:31:35.456405Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), os.path.join(config['save_dir'], f\"{config['name']}_model_{now}.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-09T09:31:35.457059Z",
     "iopub.status.idle": "2025-01-09T09:31:35.457290Z",
     "shell.execute_reply": "2025-01-09T09:31:35.457198Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "save_metrics(config['save_dir'], now, train_losses, train_dcs, valid_losses, valid_dcs)\n",
    "epochs_list = list(range(1, config['epochs'] + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize post-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-09T09:31:35.457975Z",
     "iopub.status.idle": "2025-01-09T09:31:35.458212Z",
     "shell.execute_reply": "2025-01-09T09:31:35.458113Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_list, train_losses, label='Training Loss')\n",
    "plt.plot(epochs_list, valid_losses, label='Validation Loss')\n",
    "plt.xticks(ticks=list(range(1, config['epochs'] + 1, 1)))\n",
    "plt.title('Loss over epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_list, train_dcs, label='Training DICE')\n",
    "plt.plot(epochs_list, valid_dcs, label='Validation DICE')\n",
    "plt.xticks(ticks=list(range(1, config['epochs'] + 1, 1)))\n",
    "plt.title('DICE Coefficient over epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('DICE')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6444376,
     "sourceId": 10400638,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "torch_cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
